\documentclass[a4paper, 12pt]{report}
\addtolength{\evensidemargin}{-5.3cm}
\addtolength{\oddsidemargin}{-1,5cm}
\addtolength{\textwidth}{3cm}
\addtolength{\textheight}{5cm}
\addtolength{\topmargin}{-1cm}
\addtolength{\headheight}{-2cm}
\usepackage[utf8]{inputenc}
\usepackage{amsthm,amsfonts}
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{graphicx}

\begin{document}
\title{Neural Networks}
\maketitle

\section{Neural Networks}
Let a network with $L$ layers. At each layer: $\ell\in\{1, 2, \cdots, L\}$, we have weights and bias:
\begin{equation}
\begin{array}{ll}
z^{[\ell]} &= W^{[\ell]} a^{[\ell-1]} + b^{[\ell]} \\
a^{[\ell]} &= g^{[\ell]}(z^{[\ell]})
\end{array}
\end{equation}

If at layer $\ell$, let the number of units at such layer be denoted by $n^{[\ell]}$, then it makes sense:
\begin{equation}
a^{[\ell]}\in\mathbb{R}^{n^{[\ell]}},\quad\quad
b^{[\ell]}\in\mathbb{R}^{n^{[\ell]}},\quad\quad
W^{[\ell]}\in\mathbb{R}^{n^{[\ell]}\times n^{[\ell-1]}}\quad\quad
g^{[\ell]}: \mathbb{R}\to\mathbb{R}
\end{equation}

The only exception for $g$ is the last layer, which might be $g^{[L]}: \mathbb{R}^{n^{[L]}}\to\mathbb{R}^{n^{[L]}}$ (example, softmax activation).
The network prediction $\hat y$ is precisely the output at the last layer, $\hat y = a^{[L]}$. From there, define the loss function, comparing the predicted value $\hat y$ with the actual value $y$:
\begin{equation}
\begin{array}{lll}
\mathcal L(y, \hat y) &= -y\log\hat y - (1-y)\log(1-\hat y)\quad&\text{(Logistic loss)} \\
\mathcal L(y, \hat y) &= -\sum_i y_i\log\hat y_i\quad&\text{(Categorical cross-entropy loss)} \\
\mathcal L(y, \hat y) &= \frac{1}{2}(y-\hat y)^T(y-\hat y)&\text{(Quadratic Loss)} \\
\end{array}
\end{equation}

Note categorical cross-entropy loss is exactly the cross-entropy between two distributions. Also note it is a generalization of logistic loss for more than two labels. If there are $m$ training examples, define the cost function:
\begin{equation}
J(W,b) = \sum_m\mathcal L(y_m, \hat y_m)
\end{equation}

The immediate gradients are thus:
\begin{equation}
\begin{array}{ll}
\displaystyle\frac{\partial z_i^{[\ell]}}{\partial W^{[\ell]}_{jk}}
&=\displaystyle\frac{\partial}{\partial W^{[\ell]}_{jk}}\left[\sum_l W^{[\ell]}_{il} a^{[\ell-1]}_{l}+b^{[\ell]}_i\right]
=\displaystyle\sum_l\frac{\partial W^{[\ell]}_{il}}{\partial W^{[\ell]}_{jk}} a^{[\ell-1]}_{l}
=\displaystyle\sum_l\delta_{ij}\delta_{lk} a^{[\ell-1]}_{l} = a^{[\ell-1]}_k\delta_{ij}
\\
\displaystyle\frac{\partial z^{[\ell]}_i}{\partial a^{[\ell-1]}_j}
&=\displaystyle\frac{\partial}{\partial a^{[\ell-1]}_j}\left[\sum_l W^{[\ell]}_{il} a^{[\ell-1]}_l + b^{[\ell]}_j\right]
= \displaystyle\sum_l W_{il}^{[\ell]}\frac{\partial a_l^{[\ell-1]}}{\partial a_j^{[\ell-1]}}
= \displaystyle\sum_l W_{il}^{[\ell]}\delta_{il}
= W_{ij}^{[\ell]}
\\
\displaystyle\frac{\partial z^{[\ell]}_i}{\partial b^{[\ell]}_j}
&=\displaystyle\frac{\partial}{\partial b^{[\ell]}_j}\left[\sum_l W^{[\ell]}_{il} a^{[\ell-1]}_l + b^{[\ell]}_i\right]
= \displaystyle\frac{\partial b_i^{[\ell-1]}}{\partial b_j^{[\ell-1]}}
= \displaystyle\delta_{ij}
\end{array}
\end{equation}


Consider $J_{ij}^{[\ell]}$ is the jacobian of matrix from the activation $g^{[\ell]}$, then, a single backwards step can be computed:
\begin{equation}
\frac{\partial a_i^{[\ell]}}{\partial a_j^{[\ell-1]}}
=\sum_k\frac{\partial a_i^{[\ell]}}{\partial z_k^{[\ell]}}\frac{\partial z_k^{[\ell]}}{\partial a_j^{[\ell-1]}}
=\sum_k J_{ik}^{[\ell]} W_{kj}^{[\ell]}
\quad\implies\quad
\frac{\partial a^{[\ell]}}{\partial a^{[\ell-1]}} = J^{[\ell]} W^{[\ell]}
\end{equation}


From there, because $a^{[\ell]} = g^{[\ell]}\left(W^{[\ell]} a^{[\ell-1]} + b^{[\ell]}\right)$, backpropagation soon follows:
\begin{equation}
\frac{\partial\mathcal L}{\partial a_i^{[\ell]}} = 
\sum_{k_0,k_1,\cdots k_{L-\ell+1}}
\frac{\partial\mathcal L}{\partial a_{k_0}^{[L]}}
\frac{\partial a_{k_0}^{[L]}}{\partial a_{k_1}^{[L-1]}}
\frac{\partial a_{k_1}^{[L-1]}}{\partial a_{k_2}^{[L-2]}}
\cdots
\frac{\partial a_{k_{L-\ell+1}}^{[\ell+1]}}{\partial a_{k_{L-\ell}}^{[\ell]}}
=
\sum_{k_0,k_1,\cdots k_{L-\ell}}
\frac{\partial\mathcal L}{\partial a_{k_0}^{[L]}}
\prod_{l=0}^{L-\ell+1}
\frac{\partial a_{k_{L-l+1}}^{[l+1]}}{\partial a_{k_{L-l}}^{[l]}}
\end{equation}

\begin{equation}
\frac{\partial\mathcal L}{\partial W_{ij}^{[\ell]}}
=\sum_{k}\sum_l\frac{\partial\mathcal L}{\partial a_k^{[\ell]}}\frac{\partial a_k^{[\ell]}}{\partial z_l^{[\ell]}}\frac{\partial z_l^{[\ell]}}{\partial W_{ij}^{[\ell]}}
=\sum_k\sum_l\frac{\partial\mathcal L}{\partial a_k^{[\ell]}} J_{kl}^{[\ell]}\delta_{li}a_j^{[\ell-1]}
= \sum_k\frac{\partial\mathcal L}{\partial a_k^{[\ell]}} J_{ki}^{[\ell]} a_j^{[\ell-1]}
\end{equation}

\begin{equation}
\frac{\partial\mathcal L}{\partial b_i^{[\ell]}}
=\sum_{k}\sum_l\frac{\partial\mathcal L}{\partial a_k^{[\ell]}}\frac{\partial a_k^{[\ell]}}{\partial z_l^{[\ell]}}\frac{\partial z_l^{[\ell]}}{\partial b_i^{[\ell]}}
=\sum_k\sum_l\frac{\partial\mathcal L}{\partial a_k^{[\ell]}} J_{kl}^{[\ell]}\delta_{li}
= \sum_k\frac{\partial\mathcal L}{\partial a_k^{[\ell]}} J_{ki}^{[\ell]}
\end{equation}



\newpage
\section{Recurrent Networks (RNN)}
A recurrent unit with a state $h$, input $x$ and output $\hat y$, follows the equation:
\begin{equation}
\begin{array}{ll}
h^{\langle t\rangle} &= g_h\left(W_{hh} h^{\langle t-1\rangle} + W_{hx} x^{\langle t\rangle} + b_h\right) \\
\hat y^{\langle t\rangle} &= g_y \left(W_{yh} h^{\langle t\rangle} + b_y\right)
\end{array}
\end{equation}

Such that:
\begin{equation}
\begin{array}{c}
x\in\mathbb{R}^{n_x},\quad\quad
h\in\mathbb{R}^{n_h},\quad\quad
y\in\mathbb{R}^{n_y},\quad\quad
b_a\in\mathbb{R}^{n_a},\quad\quad
b_y\in\mathbb{R}^{n_y},\quad\quad
\\
W_{ax}\in\mathbb{R}^{n_a\times n_x}\quad\quad
W_{aa}\in\mathbb{R}^{n_a\times n_a}\quad\quad
W_{ya}\in\mathbb{R}^{n_y\times n_a}\quad\quad
\\
\end{array}
\end{equation}

The loss function at time $t$:
\begin{equation}
\begin{array}{lll}
\mathcal L^{\langle t\rangle}(y^{\langle t\rangle}, \hat y^{\langle t\rangle}) &= -y^{\langle t\rangle}\log\hat y^{\langle t\rangle} - (1-y^{\langle t\rangle})\log(1-\hat y^{\langle t\rangle})\quad&\text{(Logistic loss)} \\
\mathcal L^{\langle t\rangle}(y^{\langle t\rangle}, \hat y^{\langle t\rangle}) &= -\sum_i y_i^{\langle t\rangle}\log\hat y_i^{\langle t\rangle}\quad&\text{(Categorical cross-entropy)} \\
\mathcal L^{\langle t\rangle}(y^{\langle t\rangle}, \hat y^{\langle t\rangle}) &= \frac{1}{2}(y^{\langle t\rangle}-\hat y^{\langle t\rangle})^T(y^{\langle t\rangle}-\hat y^{\langle t\rangle})&\text{(Quadratic Loss)} \\
\end{array}
\end{equation}

The full loss function are then defined as:
\begin{equation}
\mathcal L(\hat y, y) = \sum_{t=1}^{T}\mathcal L^{\langle t\rangle}(y^{\langle t\rangle}, \hat y^{\langle t\rangle}),\quad\quad
\end{equation}
\end{document}


